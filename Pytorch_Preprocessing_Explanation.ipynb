{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch-Preprocessing-Explanation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOMau7fBnGMNnS19jR0beGt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manvendra7/TSAI_END2.0-Assignment-8/blob/main/Pytorch_Preprocessing_Explanation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ns8jeZcK2yH"
      },
      "source": [
        "## What are Pytorch Datasets?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx68nOaO_tK8"
      },
      "source": [
        "Loading the Dataset -\n",
        "\n",
        "PyTorch provides two data primitives: torch.utils.data.DataLoader and torch.utils.data.Dataset that allows us to use preloaded datasets as well as your own data. Dataset stores the samples and their corresponding labels, and DataLoader wraps an iterable around the Dataset to enable easy access to the samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vx7JhEp_3yZ"
      },
      "source": [
        "What are pytorch Datasets?\n",
        "\n",
        "PyTorch Datasets are just things that have a length and are indexable so that len(dataset) will work and dataset[index] will return a tuple of (x,y).\n",
        "\n",
        "Pytorchâ€™s data sets have \"dunder/magic methods\" __getitem__ (for dataset[index] functionality) and __len__ (for len(dataset) functionality)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haVoAmAAU50B"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuxuYFSlCpfN"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82N2QTdnU53Z",
        "outputId": "b356804c-adeb-4995-c8a2-d05b0beeec6e"
      },
      "source": [
        "class CustomTextDataset(Dataset):\n",
        "\n",
        "  \"Custom Dataset class to load QnA data\"\n",
        "\n",
        "  def __init__(self,src, tgt,path):\n",
        "\n",
        "      self.df = pd.read_csv(path,sep='\\t',encoding='iso-8859-1')   # read the data\n",
        "      self.df = self.df[['Question','Answer']].dropna().reset_index(drop=True)   # drop the null values from the dataset\n",
        "\n",
        "      self.src = self.df[src]   # source column or Questions\n",
        "      self.tgt = self.df[tgt]   # target column or Answers\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.tgt)   # return the length of dataset\n",
        "\n",
        "  def __getitem__(self, idx):  # returns a dictionary of questions and answers\n",
        "      src = self.src[idx]\n",
        "      tgt = self.tgt[idx]\n",
        "      sample = {\"SRC\":src,\"TGT\" :tgt}   \n",
        "      return sample\n",
        "\n",
        "data = CustomTextDataset('Question','Answer','/content/full_question_answer_data.txt')\n",
        "next(iter(data))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SRC': 'Was Abraham Lincoln the sixteenth President of the United States?',\n",
              " 'TGT': 'yes'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y37uUgWxCplr",
        "outputId": "da536137-c6e2-4e8d-a568-92c1ef489228"
      },
      "source": [
        "list(DataLoader(data))[:11]"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'SRC': ['Was Abraham Lincoln the sixteenth President of the United States?'],\n",
              "  'TGT': ['yes']},\n",
              " {'SRC': ['Was Abraham Lincoln the sixteenth President of the United States?'],\n",
              "  'TGT': ['Yes.']},\n",
              " {'SRC': ['Did Lincoln sign the National Banking Act of 1863?'],\n",
              "  'TGT': ['yes']},\n",
              " {'SRC': ['Did Lincoln sign the National Banking Act of 1863?'],\n",
              "  'TGT': ['Yes.']},\n",
              " {'SRC': ['Did his mother die of pneumonia?'], 'TGT': ['no']},\n",
              " {'SRC': ['Did his mother die of pneumonia?'], 'TGT': ['No.']},\n",
              " {'SRC': [\"How many long was Lincoln's formal education?\"],\n",
              "  'TGT': ['18 months']},\n",
              " {'SRC': [\"How many long was Lincoln's formal education?\"],\n",
              "  'TGT': ['18 months.']},\n",
              " {'SRC': ['When did Lincoln begin his political career?'], 'TGT': ['1832']},\n",
              " {'SRC': ['When did Lincoln begin his political career?'], 'TGT': ['1832.']},\n",
              " {'SRC': ['What did The Legal Tender Act of 1862 establish?'],\n",
              "  'TGT': ['the United States Note, the first paper currency in United States history']}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pOtko1WGU4fI",
        "outputId": "b27cbbeb-0d00-473d-ab9e-8db760b1d947"
      },
      "source": [
        "for idx,sample in enumerate(DataLoader(data)):\n",
        "  print(idx)\n",
        "  print(sample)\n",
        "  print('-'*100)\n",
        "\n",
        "  if idx == 5:\n",
        "    break"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "{'SRC': ['Was Abraham Lincoln the sixteenth President of the United States?'], 'TGT': ['yes']}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "1\n",
            "{'SRC': ['Was Abraham Lincoln the sixteenth President of the United States?'], 'TGT': ['Yes.']}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "2\n",
            "{'SRC': ['Did Lincoln sign the National Banking Act of 1863?'], 'TGT': ['yes']}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "3\n",
            "{'SRC': ['Did Lincoln sign the National Banking Act of 1863?'], 'TGT': ['Yes.']}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "4\n",
            "{'SRC': ['Did his mother die of pneumonia?'], 'TGT': ['no']}\n",
            "----------------------------------------------------------------------------------------------------\n",
            "5\n",
            "{'SRC': ['Did his mother die of pneumonia?'], 'TGT': ['No.']}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71VFbq-KFDvp",
        "outputId": "e851dc9b-e8f2-47e8-dedd-cab1df4b04d7"
      },
      "source": [
        "bat_size = 2\n",
        "DL_DS = DataLoader(data, batch_size=bat_size)\n",
        "\n",
        "# loop through each batch in the DataLoader object\n",
        "for (idx,batch) in enumerate(DL_DS):\n",
        "\n",
        "    # Print the 'text' data of the batch\n",
        "     print(idx, 'SRC data: ', batch['SRC'], '\\n')\n",
        "\n",
        "    # Print the 'class' data of batch\n",
        "     print(idx, 'TGT data: ', batch['TGT'], '\\n')\n",
        "\n",
        "     if idx == 5:\n",
        "       break"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 SRC data:  ['Was Abraham Lincoln the sixteenth President of the United States?', 'Was Abraham Lincoln the sixteenth President of the United States?'] \n",
            "\n",
            "0 TGT data:  ['yes', 'Yes.'] \n",
            "\n",
            "1 SRC data:  ['Did Lincoln sign the National Banking Act of 1863?', 'Did Lincoln sign the National Banking Act of 1863?'] \n",
            "\n",
            "1 TGT data:  ['yes', 'Yes.'] \n",
            "\n",
            "2 SRC data:  ['Did his mother die of pneumonia?', 'Did his mother die of pneumonia?'] \n",
            "\n",
            "2 TGT data:  ['no', 'No.'] \n",
            "\n",
            "3 SRC data:  [\"How many long was Lincoln's formal education?\", \"How many long was Lincoln's formal education?\"] \n",
            "\n",
            "3 TGT data:  ['18 months', '18 months.'] \n",
            "\n",
            "4 SRC data:  ['When did Lincoln begin his political career?', 'When did Lincoln begin his political career?'] \n",
            "\n",
            "4 TGT data:  ['1832', '1832.'] \n",
            "\n",
            "5 SRC data:  ['What did The Legal Tender Act of 1862 establish?', 'What did The Legal Tender Act of 1862 establish?'] \n",
            "\n",
            "5 TGT data:  ['the United States Note, the first paper currency in United States history', 'The United States Note, the first paper currency in United States history.'] \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NUhJfyVXDA4"
      },
      "source": [
        "In machine learning or deep learning text needs to be cleaned and turned in to vectors prior to training. DataLoader has a handy parameter called collate_fn. This parameter allows you to create separate data processing functions and will apply the processing within that function to the data before it is output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rdlIs8teepf"
      },
      "source": [
        "need to use and learn collate and put it in"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuluaN6ubxJx",
        "outputId": "87096145-37a6-49c3-a00d-58cd31fe8b0e"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "python -m spacy download de"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9MB)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py): started\n",
            "  Building wheel for de-core-news-sm (setup.py): finished with status 'done'\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.2.5-cp37-none-any.whl size=14907055 sha256=801c86f29c6a4f1e61b7b9a1b042a9143182571eada9ae9a4c0cb166a1be2d21\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-62q4ej5w/wheels/ba/3f/ed/d4aa8e45e7191b7f32db4bfad565e7da1edbf05c916ca7a1ca\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.2.5\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2mâœ” Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esVjcKeAcsZj"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from typing import Iterable, List"
      ],
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEogSu4QU6P3",
        "outputId": "804929d2-c9ac-4770-a77c-de409d313039"
      },
      "source": [
        "###########################################################################################\n",
        "###########################################################################################\n",
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
        "tokens\n",
        "#['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqmh6u8xc3kX"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import AG_NEWS\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "vocab(['hello','we','are','languagecorpus'])\n",
        "\n",
        "# vocab(['hello','we','are','languagecorpus'])"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JMXG1tEaw8T",
        "outputId": "93b711f8-dad1-43b7-d7a5-078765a6a797"
      },
      "source": [
        ""
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[12544, 507, 42, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqU5lBoGemjv",
        "outputId": "24192cc4-df9f-4fc1-e7a5-39d2bd34c5cc"
      },
      "source": [
        "from torchtext.datasets import Multi30k\n",
        "train_iter = Multi30k(split='train')\n",
        "next(iter(train_iter))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Zwei junge weiÃŸe MÃ¤nner sind im Freien in der NÃ¤he vieler BÃ¼sche.\\n',\n",
              " 'Two young, White males are outside near many bushes.\\n')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvUVXRpvemnI"
      },
      "source": [
        "SRC_LANGUAGE = 'de'\n",
        "TGT_LANGUAGE = 'en'\n",
        "token_transform = {}\n",
        "vocab_transform = {}"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9K-oq-cfemq7"
      },
      "source": [
        "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy',language='de')\n",
        "token_transform[TGT_LANGUAGE] = get_tokenizer('spacy',language='en')\n",
        "\n",
        "def yield_tokens(data_iter:Iterable,language:str):\n",
        "  language_index = {SRC_LANGUAGE:0,TGT_LANGUAGE:1}\n",
        "  for data_sample in data_iter:\n",
        "    yield token_transform[language](data_sample[language_index[language]])"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8eM7DvbWemt2"
      },
      "source": [
        "# Define special symbols and indices\n",
        "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
        "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
        "special_symbols = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tysyw4n9emxk"
      },
      "source": [
        "for ln in [SRC_LANGUAGE,TGT_LANGUAGE]:\n",
        "  train_iter = Multi30k(split='train')\n",
        "  vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train_iter,ln),\n",
        "                                                  min_freq=1,\n",
        "                                                  specials=special_symbols,\n",
        "                                                  special_first = True)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnNJD5RFem1r"
      },
      "source": [
        "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
        "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "  vocab_transform[ln].set_default_index(UNK_IDX)"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTtSv3mUem3t",
        "outputId": "32c6e08d-10dd-4cda-f88a-287fc30c9b5b"
      },
      "source": [
        "vocab_transform[TGT_LANGUAGE](['hello','i','am','vocab','transform'])"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5466, 2590, 3427, 0, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uROt4asEK9JY"
      },
      "source": [
        "# def load_data(test_split, batch_size):\n",
        "#     \"\"\"Loads the data\"\"\"\n",
        "#     sonar_dataset = CustomTextDataset('./sonar.all-data')\n",
        "#     # Create indices for the split\n",
        "#     dataset_size = len(sonar_dataset)\n",
        "#     test_size = int(test_split * dataset_size)\n",
        "#     train_size = dataset_size - test_size\n",
        "\n",
        "#     train_dataset, test_dataset = random_split(sonar_dataset,\n",
        "#                                                [train_size, test_size])\n",
        "\n",
        "#     train_loader = DataLoader(\n",
        "#         train_dataset.dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True)\n",
        "#     test_loader = DataLoader(\n",
        "#         test_dataset.dataset,\n",
        "#         batch_size=batch_size,\n",
        "#         shuffle=True)\n",
        "\n",
        "#     return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSwajX4yAoCA"
      },
      "source": [
        "## Understanding the Collate function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dnBiHRQcsg8"
      },
      "source": [
        "# pytorch's default collate function taken from https://github.com/pytorch/pytorch/blob/master/torch/utils/data/_utils/collate.py\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import collections\n",
        "from torch._six import string_classes\n",
        "\n",
        "np_str_obj_array_pattern = re.compile(r'[SaUO]')\n",
        "\n",
        "\n",
        "def default_convert(data):\n",
        "    r\"\"\"Converts each NumPy array data field into a tensor\"\"\"\n",
        "    elem_type = type(data)\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        # array of string classes and object\n",
        "        if elem_type.__name__ == 'ndarray' \\\n",
        "                and np_str_obj_array_pattern.search(data.dtype.str) is not None:\n",
        "            return data\n",
        "        return torch.as_tensor(data)\n",
        "    elif isinstance(data, collections.abc.Mapping):\n",
        "        return {key: default_convert(data[key]) for key in data}\n",
        "    elif isinstance(data, tuple) and hasattr(data, '_fields'):  # namedtuple\n",
        "        return elem_type(*(default_convert(d) for d in data))\n",
        "    elif isinstance(data, collections.abc.Sequence) and not isinstance(data, string_classes):\n",
        "        return [default_convert(d) for d in data]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "\n",
        "default_collate_err_msg_format = (\n",
        "    \"default_collate: batch must contain tensors, numpy arrays, numbers, \"\n",
        "    \"dicts or lists; found {}\")\n",
        "\n",
        "\n",
        "def default_collate(batch):\n",
        "    r\"\"\"Puts each data field into a tensor with outer dimension batch size\"\"\"\n",
        "\n",
        "    elem = batch[0]\n",
        "    elem_type = type(elem)\n",
        "    if isinstance(elem, torch.Tensor):\n",
        "        out = None\n",
        "        if torch.utils.data.get_worker_info() is not None:\n",
        "            # If we're in a background process, concatenate directly into a\n",
        "            # shared memory tensor to avoid an extra copy\n",
        "            numel = sum([x.numel() for x in batch])\n",
        "            storage = elem.storage()._new_shared(numel)\n",
        "            out = elem.new(storage)\n",
        "        return torch.stack(batch, 0, out=out)\n",
        "    elif elem_type.__module__ == 'numpy' and elem_type.__name__ != 'str_' \\\n",
        "            and elem_type.__name__ != 'string_':\n",
        "        if elem_type.__name__ == 'ndarray' or elem_type.__name__ == 'memmap':\n",
        "            # array of string classes and object\n",
        "            if np_str_obj_array_pattern.search(elem.dtype.str) is not None:\n",
        "                raise TypeError(default_collate_err_msg_format.format(elem.dtype))\n",
        "\n",
        "            return default_collate([torch.as_tensor(b) for b in batch])\n",
        "        elif elem.shape == ():  # scalars\n",
        "            return torch.as_tensor(batch)\n",
        "    elif isinstance(elem, float):\n",
        "        return torch.tensor(batch, dtype=torch.float64)\n",
        "    elif isinstance(elem, int):\n",
        "        return torch.tensor(batch)\n",
        "    elif isinstance(elem, string_classes):\n",
        "        return batch\n",
        "    elif isinstance(elem, collections.abc.Mapping):\n",
        "        return {key: default_collate([d[key] for d in batch]) for key in elem}\n",
        "    elif isinstance(elem, tuple) and hasattr(elem, '_fields'):  # namedtuple\n",
        "        return elem_type(*(default_collate(samples) for samples in zip(*batch)))\n",
        "    elif isinstance(elem, collections.abc.Sequence):\n",
        "        # check to make sure that the elements in batch have consistent size\n",
        "        it = iter(batch)\n",
        "        elem_size = len(next(it))\n",
        "        if not all(len(elem) == elem_size for elem in it):\n",
        "            raise RuntimeError('each element in list of batch should be of equal size')\n",
        "        transposed = zip(*batch)\n",
        "        return [default_collate(samples) for samples in transposed]\n",
        "\n",
        "    raise TypeError(default_collate_err_msg_format.format(elem_type))"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00R6kJx4csqk",
        "outputId": "06395654-6949-4016-d34a-02cd62d742d2"
      },
      "source": [
        "item_list = [1,2,3,4,5]\n",
        "default_collate(item_list)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0LHu0-Icsx0",
        "outputId": "9d708d51-1362-4de0-88da-83f38d177c21"
      },
      "source": [
        "item_list = ([1,2,3,4,5],[6,7,8,9,10])\n",
        "default_collate(item_list)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 6]),\n",
              " tensor([2, 7]),\n",
              " tensor([3, 8]),\n",
              " tensor([4, 9]),\n",
              " tensor([ 5, 10])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn_zAw-rerXK",
        "outputId": "d63f97a6-51d1-428f-94af-0a98353d61a0"
      },
      "source": [
        "item_list = [(1,2),(3,4),(5,6),(7,8)]\n",
        "default_collate(item_list)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 3, 5, 7]), tensor([2, 4, 6, 8])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPENDJ9LfSbz",
        "outputId": "2975ae5c-ce46-4e4c-cd77-44773266d7c8"
      },
      "source": [
        "item_list = [[1,2,3],[3,4,5],[5,6,7],[7,8,9]]\n",
        "default_collate(item_list)"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 3, 5, 7]), tensor([2, 4, 6, 8]), tensor([3, 5, 7, 9])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JmrnOZMf0yn",
        "outputId": "9271b07b-17cb-41c1-c03e-97c7ce2ca2c5"
      },
      "source": [
        "import torch\n",
        "def our_own_collate(data):\n",
        "\n",
        " # from [[x1,x2,y],[...]] to [tensor([[x1,x2],.....]) tensor([y,.....])] \n",
        "\n",
        " xs = [[data_item[0],data_item[1]] for data_item in data]\n",
        " y = [data_item[2] for data_item in data]\n",
        "\n",
        " return torch.tensor(xs),torch.tensor(y)\n",
        "\n",
        "item_list = [[1,2,3],[3,4,5],[5,6,7],[7,8,9]]\n",
        "our_own_collate(item_list)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1, 2],\n",
              "         [3, 4],\n",
              "         [5, 6],\n",
              "         [7, 8]]), tensor([3, 5, 7, 9]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po9nGzuGBKkZ"
      },
      "source": [
        "## Custom collate function for loading sequential dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ppXHEuwem6w"
      },
      "source": [
        "######################################################################\n",
        "# Collation\n",
        "# ---------\n",
        "#   \n",
        "# As seen in the ``Data Sourcing and Processing`` section, our data iterator yields a pair of raw strings. \n",
        "# We need to convert these string pairs into the batched tensors that can be processed by our ``Seq2Seq`` network \n",
        "# defined previously. Below we define our collate function that convert batch of raw strings into batch tensors that\n",
        "# can be fed directly into our model.   \n",
        "#\n",
        "\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            txt_input = transform(txt_input)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]), \n",
        "                      torch.tensor(token_ids), \n",
        "                      torch.tensor([EOS_IDX])))\n",
        "\n",
        "# src and tgt language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
        "                                               vocab_transform[ln], #Numericalization\n",
        "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMs-2ivMi4XL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}